{
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Advanced Missing Data Imputation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import preprocessing\nimport math\nimport numpy as np\n\n# Enable inline plotting\n%matplotlib inline",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Advanced Regression Models\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv('College-MISSING.csv')\nprint(df.head())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's predict graduation rate (Grad.Rate) based on the other variables. So Grad.Rate will be our outcome (y) and the other variables will be our features (X). \n\nBut first we will remove missing data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Removing Missing Data\n\nWe will remove any rows with missing (NA) data, in order to fit our advanced regression models.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_complete = df.dropna(axis=0, how='any')\nprint(df_complete.head())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Then we will divide that into features (X) and outcomes (y), as before. We also add a line of code that scales the variables (preprocessing.scale(X)), as neural networks usually perform better when all the features are on roughly the same scale. If you remove this, you will likely see performance go down. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = df_complete.drop(['College', 'Private', 'Grad.Rate'], axis=1)\nX = pd.DataFrame(preprocessing.scale(X), columns=X.columns)\nprint(\"Here are the features (X):\")\nprint(X.head())\n\nprint(\"\\n\\nHere is the outcome variable (y):\")\ny = df_complete['Grad.Rate']\nprint(y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(df_complete.head())",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Advanced Regression Models in sklearn",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We create an instance of the MLPRegressor class in sklearn. We need to specify how many hidden layers it should have and how many neurons in each of those layers. In this case, it's three hidden layers, with sizes of 100, 100, and 50, respectively. The other arguments specify which training algorithm it should use for learning the neural network weights, and the maximum number of iterations it should try.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lm1 = MLPRegressor(hidden_layer_sizes=(100,100,50), solver='lbfgs', max_iter=500, random_state=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We then fit the model to the training data. Thankfully, sklearn provides a consistent API for the various machine learning models. So training and prediction is just the same as it was before. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lm1.fit(X, y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's also create a kNN regression model. We can specify the value of _k_ or use the default of 5.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lm2 = KNeighborsRegressor(10)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Then we fit that model to the data as well.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lm2.fit(X,y)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Imputation\n\nThen we want to get predictions on the full dataset, including the rows that had missing (NA) data, so that we can impute the missing values using our trained regression model. \n\nSo we need to get the features (X) for the entire dataset, not just the complete cases.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_all = df.drop(['College', 'Private', 'Grad.Rate'], axis=1)\nX_all = pd.DataFrame(preprocessing.scale(X_all), columns=X_all.columns)\n\npreds1 = lm1.predict(X_all)\n\npreds2 = lm2.predict(X_all)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Or we could get predictions for just the records where Grad.Rate was missing:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing = df['Grad.Rate'].isnull()\n\npreds_missing1 = lm1.predict(X_all.loc[missing, :])\n\npreds_missing2 = lm2.predict(X_all.loc[missing, :])\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Evaluation\n\nLet's do a similar step as you did in the first part of your lab assignment last week. We will look at the gold-standard Grad.Rate values in _College.csv_ and see how our predictions compare. But this time we will only look at the subset of the data that was missing.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "gs_df = pd.read_csv('College.csv')\ngs_grad = gs_df.loc[missing, 'Grad.Rate']\n\nplt.figure()\nplt.scatter(preds_missing1, gs_grad)\nplt.xlabel(\"Predicted Grad Rate (NN)\")\nplt.ylabel(\"Actual Grad Rate\")\nplt.show()\n\nplt.figure()\nplt.scatter(preds_missing2, gs_grad)\nplt.xlabel(\"Predicted Grad Rate (kNN)\")\nplt.ylabel(\"Actual Grad Rate\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "It can be difficult to assess which approach is doing better just by looking at the scatter plots. \n\nLet's calculate the mean-squared error (MSE) for each.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mse = sum((gs_grad - preds_missing1)**2) / len(preds_missing1)\nprint(\"MSE with MLP regressor: \", mse)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "mse = sum((gs_grad - preds_missing2)**2) / len(preds_missing2)\nprint(\"MSE with kNN regressor: \", mse)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}